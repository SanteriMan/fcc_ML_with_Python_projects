{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of fcc_predict_health_costs_with_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanteriMan/freecodecamp_ML_projects/blob/master/Copy_of_fcc_predict_health_costs_with_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9TX15KOkPBV",
        "colab_type": "text"
      },
      "source": [
        "*Note: You are currently reading this using Google Colaboratory which is a cloud-hosted version of Jupyter Notebook. This is a document containing both text cells for documentation and runnable code cells. If you are unfamiliar with Jupyter Notebook, watch this 3-minute introduction before starting this challenge: https://www.youtube.com/watch?v=inN8seMm7UI*\n",
        "\n",
        "---\n",
        "\n",
        "In this challenge, you will predict healthcare costs using a regression algorithm.\n",
        "\n",
        "You are given a dataset that contains information about different people including their healthcare costs. Use the data to predict healthcare costs based on new data.\n",
        "\n",
        "The first two cells of this notebook import libraries and the data.\n",
        "\n",
        "Make sure to convert categorical data to numbers. Use 80% of the data as the `train_dataset` and 20% of the data as the `test_dataset`.\n",
        "\n",
        "`pop` off the \"expenses\" column from these datasets to create new datasets called `train_labels` and `test_labels`. Use these labels when training your model.\n",
        "\n",
        "Create a model and train it with the `train_dataset`. Run the final cell in this notebook to check your model. The final cell will use the unseen `test_dataset` to check how well the model generalizes.\n",
        "\n",
        "To pass the challenge, `model.evaluate` must return a Mean Absolute Error of under 3500. This means it predicts health care costs correctly within $3500.\n",
        "\n",
        "The final cell will also predict expenses using the `test_dataset` and graph the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rRo8oNqZ-Rj",
        "colab": {}
      },
      "source": [
        "# Import libraries. You may or may not use all of these.\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiX2FI4gZtTt",
        "colab": {}
      },
      "source": [
        "# Import data\n",
        "dataset_path = keras.utils.get_file(\"insurance.csv\", \"https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\")\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LcopvQh3X-kX",
        "colab": {}
      },
      "source": [
        "dataset['sex'] = pd.Categorical(dataset['sex'])\n",
        "dataset['sex'] = dataset['sex'].cat.codes\n",
        "dataset['smoker'] = pd.Categorical(dataset['smoker'])\n",
        "dataset['smoker'] = dataset['smoker'].cat.codes\n",
        "dataset['region'] = pd.Categorical(dataset['region'])\n",
        "dataset['region'] = dataset['region'].cat.codes\n",
        "dataset.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGCY8Zab0Uvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "train_labels = train_dataset.pop(\"expenses\")\n",
        "test_labels = test_dataset.pop(\"expenses\")"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypAVS2Gb3PYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_stats = train_dataset.describe()\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3E1A8_u3GLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS8Y93wb3kUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "model.compile(loss='mse',\n",
        "                optimizer='adam',\n",
        "                metrics=['mae','mse'])"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TObVmcBn3vTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab3f96ce-dabb-4670-8bb7-2532decdcbe1"
      },
      "source": [
        "history = model.fit(normed_train_data, train_labels, epochs= 100,validation_split = 0.2)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 23068984.0000 - mae: 2950.4658 - mse: 23068984.0000 - val_loss: 26360640.0000 - val_mae: 3237.8455 - val_mse: 26360640.0000\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 23005124.0000 - mae: 2942.7117 - mse: 23005124.0000 - val_loss: 26251212.0000 - val_mae: 3230.9668 - val_mse: 26251212.0000\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22977716.0000 - mae: 2954.5371 - mse: 22977716.0000 - val_loss: 26173966.0000 - val_mae: 3232.5327 - val_mse: 26173966.0000\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 22907384.0000 - mae: 2937.4185 - mse: 22907384.0000 - val_loss: 26069792.0000 - val_mae: 3209.4719 - val_mse: 26069792.0000\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 22860864.0000 - mae: 2924.1321 - mse: 22860864.0000 - val_loss: 26007816.0000 - val_mae: 3212.5967 - val_mse: 26007816.0000\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22792264.0000 - mae: 2933.6082 - mse: 22792264.0000 - val_loss: 25928110.0000 - val_mae: 3214.5991 - val_mse: 25928110.0000\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 22735884.0000 - mae: 2922.7480 - mse: 22735884.0000 - val_loss: 25833614.0000 - val_mae: 3198.5254 - val_mse: 25833614.0000\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22690642.0000 - mae: 2913.4463 - mse: 22690642.0000 - val_loss: 25702352.0000 - val_mae: 3181.5869 - val_mse: 25702352.0000\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 22632496.0000 - mae: 2899.4893 - mse: 22632496.0000 - val_loss: 25616998.0000 - val_mae: 3164.3105 - val_mse: 25616998.0000\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22569286.0000 - mae: 2897.6326 - mse: 22569286.0000 - val_loss: 25563696.0000 - val_mae: 3178.8413 - val_mse: 25563696.0000\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22525320.0000 - mae: 2907.8569 - mse: 22525320.0000 - val_loss: 25483860.0000 - val_mae: 3178.3350 - val_mse: 25483858.0000\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 22473376.0000 - mae: 2897.4971 - mse: 22473376.0000 - val_loss: 25369950.0000 - val_mae: 3150.8730 - val_mse: 25369950.0000\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22402766.0000 - mae: 2891.7207 - mse: 22402766.0000 - val_loss: 25283388.0000 - val_mae: 3154.0295 - val_mse: 25283388.0000\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 22355722.0000 - mae: 2880.8069 - mse: 22355722.0000 - val_loss: 25175402.0000 - val_mae: 3133.7029 - val_mse: 25175402.0000\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22299978.0000 - mae: 2877.2371 - mse: 22299978.0000 - val_loss: 25098780.0000 - val_mae: 3129.8774 - val_mse: 25098780.0000\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 22237852.0000 - mae: 2870.0732 - mse: 22237852.0000 - val_loss: 25055452.0000 - val_mae: 3126.9363 - val_mse: 25055452.0000\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 22204994.0000 - mae: 2873.8552 - mse: 22204994.0000 - val_loss: 24984890.0000 - val_mae: 3118.9861 - val_mse: 24984890.0000\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22137400.0000 - mae: 2859.1001 - mse: 22137400.0000 - val_loss: 24839862.0000 - val_mae: 3101.5903 - val_mse: 24839862.0000\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22098932.0000 - mae: 2847.1865 - mse: 22098932.0000 - val_loss: 24753940.0000 - val_mae: 3085.7793 - val_mse: 24753940.0000\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 22030972.0000 - mae: 2840.7900 - mse: 22030972.0000 - val_loss: 24670856.0000 - val_mae: 3087.2419 - val_mse: 24670856.0000\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21983230.0000 - mae: 2840.6189 - mse: 21983230.0000 - val_loss: 24605084.0000 - val_mae: 3095.4951 - val_mse: 24605084.0000\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21937222.0000 - mae: 2828.1326 - mse: 21937222.0000 - val_loss: 24510820.0000 - val_mae: 3069.0288 - val_mse: 24510820.0000\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21866256.0000 - mae: 2823.2236 - mse: 21866256.0000 - val_loss: 24421688.0000 - val_mae: 3069.3010 - val_mse: 24421688.0000\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 21813892.0000 - mae: 2824.0530 - mse: 21813892.0000 - val_loss: 24355462.0000 - val_mae: 3072.1565 - val_mse: 24355462.0000\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21766582.0000 - mae: 2812.9253 - mse: 21766582.0000 - val_loss: 24269268.0000 - val_mae: 3054.7693 - val_mse: 24269268.0000\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21723292.0000 - mae: 2803.2361 - mse: 21723292.0000 - val_loss: 24163532.0000 - val_mae: 3038.2703 - val_mse: 24163532.0000\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21661794.0000 - mae: 2801.3257 - mse: 21661794.0000 - val_loss: 24106414.0000 - val_mae: 3045.9014 - val_mse: 24106414.0000\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 21614106.0000 - mae: 2793.1067 - mse: 21614106.0000 - val_loss: 24035848.0000 - val_mae: 3036.3604 - val_mse: 24035848.0000\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 21567554.0000 - mae: 2793.1875 - mse: 21567554.0000 - val_loss: 23916418.0000 - val_mae: 3036.4043 - val_mse: 23916418.0000\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21525298.0000 - mae: 2787.1528 - mse: 21525298.0000 - val_loss: 23848836.0000 - val_mae: 3017.5754 - val_mse: 23848836.0000\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 21479038.0000 - mae: 2781.0034 - mse: 21479038.0000 - val_loss: 23782476.0000 - val_mae: 3014.1201 - val_mse: 23782476.0000\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21429262.0000 - mae: 2778.7581 - mse: 21429262.0000 - val_loss: 23686200.0000 - val_mae: 3008.9893 - val_mse: 23686200.0000\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 21381692.0000 - mae: 2760.3879 - mse: 21381692.0000 - val_loss: 23578928.0000 - val_mae: 2993.1431 - val_mse: 23578928.0000\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21332750.0000 - mae: 2771.8108 - mse: 21332750.0000 - val_loss: 23559668.0000 - val_mae: 3006.6277 - val_mse: 23559668.0000\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21277182.0000 - mae: 2761.6250 - mse: 21277182.0000 - val_loss: 23424098.0000 - val_mae: 2991.5461 - val_mse: 23424098.0000\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21241426.0000 - mae: 2755.1274 - mse: 21241426.0000 - val_loss: 23349610.0000 - val_mae: 2972.1309 - val_mse: 23349610.0000\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21200038.0000 - mae: 2753.9221 - mse: 21200038.0000 - val_loss: 23279006.0000 - val_mae: 2987.5667 - val_mse: 23279006.0000\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21138782.0000 - mae: 2751.7932 - mse: 21138782.0000 - val_loss: 23200740.0000 - val_mae: 2970.9199 - val_mse: 23200740.0000\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21098048.0000 - mae: 2739.7432 - mse: 21098048.0000 - val_loss: 23140104.0000 - val_mae: 2965.9216 - val_mse: 23140104.0000\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21059506.0000 - mae: 2732.1135 - mse: 21059506.0000 - val_loss: 23047610.0000 - val_mae: 2942.4675 - val_mse: 23047610.0000\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 21017330.0000 - mae: 2725.3501 - mse: 21017330.0000 - val_loss: 22990904.0000 - val_mae: 2956.5430 - val_mse: 22990906.0000\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20963528.0000 - mae: 2721.5549 - mse: 20963528.0000 - val_loss: 22903186.0000 - val_mae: 2932.5288 - val_mse: 22903186.0000\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 20949152.0000 - mae: 2731.8032 - mse: 20949152.0000 - val_loss: 22826690.0000 - val_mae: 2957.2527 - val_mse: 22826690.0000\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20899280.0000 - mae: 2706.8240 - mse: 20899280.0000 - val_loss: 22765690.0000 - val_mae: 2905.6106 - val_mse: 22765690.0000\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 20825186.0000 - mae: 2705.1721 - mse: 20825186.0000 - val_loss: 22692702.0000 - val_mae: 2933.2822 - val_mse: 22692702.0000\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20788600.0000 - mae: 2714.4304 - mse: 20788600.0000 - val_loss: 22632642.0000 - val_mae: 2931.2844 - val_mse: 22632642.0000\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20752160.0000 - mae: 2708.9893 - mse: 20752160.0000 - val_loss: 22547284.0000 - val_mae: 2909.1895 - val_mse: 22547284.0000\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20708908.0000 - mae: 2693.2832 - mse: 20708908.0000 - val_loss: 22443054.0000 - val_mae: 2906.6321 - val_mse: 22443054.0000\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 20680066.0000 - mae: 2692.2263 - mse: 20680066.0000 - val_loss: 22345956.0000 - val_mae: 2885.5190 - val_mse: 22345956.0000\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20670418.0000 - mae: 2679.6621 - mse: 20670418.0000 - val_loss: 22336378.0000 - val_mae: 2883.3181 - val_mse: 22336376.0000\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20617468.0000 - mae: 2692.2170 - mse: 20617468.0000 - val_loss: 22262772.0000 - val_mae: 2908.1230 - val_mse: 22262772.0000\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20549798.0000 - mae: 2663.1462 - mse: 20549798.0000 - val_loss: 22149288.0000 - val_mae: 2851.0388 - val_mse: 22149288.0000\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 20513636.0000 - mae: 2668.6340 - mse: 20513636.0000 - val_loss: 22132476.0000 - val_mae: 2873.5789 - val_mse: 22132476.0000\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20471124.0000 - mae: 2658.1609 - mse: 20471124.0000 - val_loss: 22020024.0000 - val_mae: 2848.4077 - val_mse: 22020024.0000\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20470264.0000 - mae: 2681.9597 - mse: 20470264.0000 - val_loss: 21986448.0000 - val_mae: 2875.8582 - val_mse: 21986448.0000\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20412262.0000 - mae: 2648.3855 - mse: 20412262.0000 - val_loss: 21869810.0000 - val_mae: 2825.2981 - val_mse: 21869810.0000\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20352858.0000 - mae: 2646.7188 - mse: 20352858.0000 - val_loss: 21840436.0000 - val_mae: 2851.4900 - val_mse: 21840436.0000\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20321160.0000 - mae: 2653.9133 - mse: 20321160.0000 - val_loss: 21762472.0000 - val_mae: 2839.0876 - val_mse: 21762472.0000\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20279262.0000 - mae: 2633.0544 - mse: 20279262.0000 - val_loss: 21659126.0000 - val_mae: 2810.5420 - val_mse: 21659126.0000\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20242220.0000 - mae: 2631.2781 - mse: 20242220.0000 - val_loss: 21617608.0000 - val_mae: 2819.7036 - val_mse: 21617608.0000\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20216224.0000 - mae: 2638.3984 - mse: 20216224.0000 - val_loss: 21543264.0000 - val_mae: 2818.1589 - val_mse: 21543264.0000\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 20178160.0000 - mae: 2621.2993 - mse: 20178160.0000 - val_loss: 21489210.0000 - val_mae: 2794.0242 - val_mse: 21489210.0000\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20138964.0000 - mae: 2614.7817 - mse: 20138964.0000 - val_loss: 21413228.0000 - val_mae: 2801.2244 - val_mse: 21413228.0000\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20116224.0000 - mae: 2628.4409 - mse: 20116224.0000 - val_loss: 21394476.0000 - val_mae: 2804.8276 - val_mse: 21394476.0000\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20072318.0000 - mae: 2618.1162 - mse: 20072318.0000 - val_loss: 21300374.0000 - val_mae: 2785.5278 - val_mse: 21300374.0000\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20043118.0000 - mae: 2599.4485 - mse: 20043118.0000 - val_loss: 21231968.0000 - val_mae: 2769.8240 - val_mse: 21231968.0000\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 20028644.0000 - mae: 2605.9954 - mse: 20028644.0000 - val_loss: 21174970.0000 - val_mae: 2787.1519 - val_mse: 21174970.0000\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19972592.0000 - mae: 2600.2268 - mse: 19972592.0000 - val_loss: 21144030.0000 - val_mae: 2760.0815 - val_mse: 21144030.0000\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19947030.0000 - mae: 2595.9336 - mse: 19947030.0000 - val_loss: 21051392.0000 - val_mae: 2765.9614 - val_mse: 21051392.0000\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19902914.0000 - mae: 2590.0940 - mse: 19902914.0000 - val_loss: 20985164.0000 - val_mae: 2748.8350 - val_mse: 20985164.0000\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19865516.0000 - mae: 2582.3003 - mse: 19865516.0000 - val_loss: 20951808.0000 - val_mae: 2748.5059 - val_mse: 20951808.0000\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19848100.0000 - mae: 2566.3945 - mse: 19848100.0000 - val_loss: 20900766.0000 - val_mae: 2743.9583 - val_mse: 20900766.0000\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19801628.0000 - mae: 2568.8352 - mse: 19801628.0000 - val_loss: 20856772.0000 - val_mae: 2748.5850 - val_mse: 20856772.0000\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19790718.0000 - mae: 2578.1694 - mse: 19790718.0000 - val_loss: 20797966.0000 - val_mae: 2740.8357 - val_mse: 20797966.0000\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19737350.0000 - mae: 2561.3157 - mse: 19737350.0000 - val_loss: 20742570.0000 - val_mae: 2748.4236 - val_mse: 20742570.0000\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19713806.0000 - mae: 2587.1255 - mse: 19713806.0000 - val_loss: 20684410.0000 - val_mae: 2751.8467 - val_mse: 20684410.0000\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19688122.0000 - mae: 2565.4751 - mse: 19688122.0000 - val_loss: 20636670.0000 - val_mae: 2724.5759 - val_mse: 20636670.0000\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19667296.0000 - mae: 2559.7678 - mse: 19667296.0000 - val_loss: 20591538.0000 - val_mae: 2715.3674 - val_mse: 20591538.0000\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19630214.0000 - mae: 2554.6401 - mse: 19630214.0000 - val_loss: 20538580.0000 - val_mae: 2728.2932 - val_mse: 20538580.0000\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19596248.0000 - mae: 2546.5359 - mse: 19596248.0000 - val_loss: 20469916.0000 - val_mae: 2702.6633 - val_mse: 20469916.0000\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19578256.0000 - mae: 2547.1924 - mse: 19578256.0000 - val_loss: 20455010.0000 - val_mae: 2723.4763 - val_mse: 20455010.0000\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19551662.0000 - mae: 2549.0754 - mse: 19551662.0000 - val_loss: 20381720.0000 - val_mae: 2690.3528 - val_mse: 20381720.0000\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19529272.0000 - mae: 2533.8499 - mse: 19529272.0000 - val_loss: 20380834.0000 - val_mae: 2702.3440 - val_mse: 20380834.0000\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19489326.0000 - mae: 2536.5784 - mse: 19489326.0000 - val_loss: 20322150.0000 - val_mae: 2705.0095 - val_mse: 20322150.0000\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19471692.0000 - mae: 2525.5369 - mse: 19471692.0000 - val_loss: 20280666.0000 - val_mae: 2684.1682 - val_mse: 20280666.0000\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19447778.0000 - mae: 2532.8782 - mse: 19447778.0000 - val_loss: 20232668.0000 - val_mae: 2689.5422 - val_mse: 20232668.0000\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19413760.0000 - mae: 2527.8606 - mse: 19413760.0000 - val_loss: 20210510.0000 - val_mae: 2686.0159 - val_mse: 20210510.0000\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19401036.0000 - mae: 2536.7065 - mse: 19401036.0000 - val_loss: 20183602.0000 - val_mae: 2692.7458 - val_mse: 20183602.0000\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19370806.0000 - mae: 2513.3333 - mse: 19370806.0000 - val_loss: 20130852.0000 - val_mae: 2664.9985 - val_mse: 20130852.0000\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19380640.0000 - mae: 2505.7581 - mse: 19380640.0000 - val_loss: 20102194.0000 - val_mae: 2645.6846 - val_mse: 20102194.0000\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19321778.0000 - mae: 2502.8560 - mse: 19321778.0000 - val_loss: 20060406.0000 - val_mae: 2672.4661 - val_mse: 20060406.0000\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19319714.0000 - mae: 2546.8523 - mse: 19319714.0000 - val_loss: 20027644.0000 - val_mae: 2678.0381 - val_mse: 20027644.0000\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19283150.0000 - mae: 2514.1382 - mse: 19283150.0000 - val_loss: 20008252.0000 - val_mae: 2671.6602 - val_mse: 20008252.0000\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19255866.0000 - mae: 2515.4893 - mse: 19255866.0000 - val_loss: 19973782.0000 - val_mae: 2662.4585 - val_mse: 19973782.0000\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19233538.0000 - mae: 2509.2778 - mse: 19233538.0000 - val_loss: 19930470.0000 - val_mae: 2652.0203 - val_mse: 19930470.0000\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19218918.0000 - mae: 2515.3516 - mse: 19218918.0000 - val_loss: 19925722.0000 - val_mae: 2663.2026 - val_mse: 19925722.0000\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19193632.0000 - mae: 2498.7161 - mse: 19193632.0000 - val_loss: 19888696.0000 - val_mae: 2626.9788 - val_mse: 19888696.0000\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19182644.0000 - mae: 2514.1006 - mse: 19182644.0000 - val_loss: 19862886.0000 - val_mae: 2652.9861 - val_mse: 19862886.0000\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 19154976.0000 - mae: 2493.5625 - mse: 19154976.0000 - val_loss: 19819352.0000 - val_mae: 2642.1572 - val_mse: 19819352.0000\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 19136622.0000 - mae: 2500.2488 - mse: 19136622.0000 - val_loss: 19801312.0000 - val_mae: 2632.6367 - val_mse: 19801312.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xe7RXH3N3CWU",
        "colab": {}
      },
      "source": [
        "# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.\n",
        "# Test model by checking how well the model generalizes using the test set.\n",
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} expenses\".format(mae))\n",
        "\n",
        "if mae < 3500:\n",
        "  print(\"You passed the challenge. Great job!\")\n",
        "else:\n",
        "  print(\"The Mean Abs Error must be less than 3500. Keep trying.\")\n",
        "\n",
        "# Plot predictions.\n",
        "test_predictions = model.predict(test_dataset).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True values (expenses)')\n",
        "plt.ylabel('Predictions (expenses)')\n",
        "lims = [0, 50000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims,lims)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}